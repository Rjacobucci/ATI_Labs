---
title: 'Statistical Learning 1: Preprocessing'
author: "Ross Jacobucci"
output: 
  pdf_document:
    fig_width: 2.5
    fig_height: 2
    fig_caption: true
---


# Resources


Kaggle -- Data mining competitions
www.kaggle.com

Andrew Ng's Machine Learning Coursera course
https://www.coursera.org/course/ml

Statistical Learning Online course
https://class.stanford.edu/courses/HumanitiesandScience/StatLearning/Winter2015/about

Based on An Introduction to Statistical Learning book -- Free
http://www-bcf.usc.edu/~gareth/ISL/

Applied Predictive Modeling 
Free book through library -- by author of caret package
http://appliedpredictivemodeling.com/

caret package tutorial:
http://topepo.github.io/caret/

JSS tutorial:
http://www.jstatsoft.org/v28/i05/paper



## Let's get started



Relevant packages:

```{r,message=FALSE}
library(caret) 
library(e1071) # for svm()
library(psych) # for describe()
library(pROC) # for roc
```


If we want an example with a continuous outcome, we can used this dataset:

```{r,message=FALSE}
library(AppliedPredictiveModeling)
data(solubility)

# Outcome variables
summary(solTestY)
summary(solTrainY)

# Predictors
# solTestX;solTestXtrans
# solTrainX;solTrainXtrans
```

Before we get started, we are going to handle missing values. A lot of statistical learning packages have problems with missing values. To take care of this, I am going to impute all missing values for the entire dataset. This is not the best strategy, but since the focus of this talk isn't on missingness, we are going to proceed.

For single imputation, I like to use k-nearest neighbors in the preProcess() from caret (method= "knnImpute")

Just for the predictors.

Also, worth centering (mean=0) and scaling(sd=1) each variable. This will speed up computation, and is necessary for many algorithms such as PCA which shows bias for differently scaled variables

But in viewing the dataset, it looks like rows where there are NA's, the rows are completely missing. Therefore, we will just remove rows of NA's


Almost all datasets that come with packages don't have missingness and are pretty clean. As an example of pre processing, we will use the HolzingerSwineford1939 dataset
```{r,message=FALSE}
# first find number of missing
# can get from summary()
# also from describe() in psych package
library(psych)
library(lavaan)
HS <- HolzingerSwineford1939
# describe(HS)
# summary(HS)
# another way
apply(apply(HS,2,is.na),2,sum)
# only one missing value
```

To center and scale, as well as impute, we can use preProcess() from caret. As an example, we will just use the x variables from HS
```{r}

pred.pre <- preProcess(HS[,7:15],method=c("center","scale","knnImpute"))
XX.pre <- predict(pred.pre,HS[,7:15])
summary(XX.pre)

# or, can just use scale() from base R
# ?scale
```

Now let's check to make sure there isn't any significant skewness or kurtosis

```{r}
describe(HS)
# nothing horrible
```

If there was significant skewness or kurtosis:

use BoxCox among others available in preProcess()

```{r,eval=FALSE}
preProc <- preProcess(XX.pre,method=c("BoxCox"))
# to see all options ?preProcess
```


# Overfitting



Demonstration what happens with so many predictors

```{r}
mat <- data.frame(matrix(0, nrow = 2000, ncol = 100))
for(i in 1:ncol(mat)) mat[,i] <- rnorm(2000)
mat$y.bin <- rbinom(1000,1,.5)
mat$y.reg <- rnorm(2000)

out3 = lm(y.reg ~ ., data= mat)
summary(out3)$r.squared
# r2 = 0.09
# summary(out3)


# use Support Vector Machines

svm.out1 <- svm(y.reg ~ ., data= mat) # from e1071
# summary(svm.out1)
f.predict <- predict(svm.out1,mat)
(pred.rsq <- cor(f.predict, mat$y.reg)**2)
```

So, we can get an R-squared of .78 just a result of having enough random predictors?

When we start to use more powerful algorithms, the propensity for over-fitting -- fitting random noise and not "real" variation -- increases drastically

We need a way to prevent over-fitting, but also want to use something powerful enough to capture meaningful effects. This is the concept of bias-variance tradeoff

Bias = how accurately are capturing the effects?
Variance = how variable are our estimated effects across partitions of the dataset?

p.36 ISLR

These concepts are best demonstrated with cross-validation


Cross-Validation -- p.71 APM
```{r}
# ?sample
set.seed(3425)
idx <- sample(2000, 1000,replace=FALSE)
train <- mat[idx,]; dim(train)
test <- mat[-idx,]; dim(test)


lm.out2 <- lm(y.reg ~., train)
# summary(lm.out2)
# r2 = 0.12
pred.testLM <- predict(lm.out2,test)
(pred.rsq <- cor(pred.testLM, test$y.reg)**2)


# same thing with svm
svm.out2 <- svm(y.reg ~., train)
# train
pred.trainSVM <- predict(svm.out2,train)
cor(pred.trainSVM, train$y.reg)**2
# test
pred.testSVM <- predict(svm.out2,test)
cor(pred.testSVM, test$y.reg)**2

```


How about if we cross-validate our real regression example?

```{r}
set.seed(3425)
sol.dat <- data.frame(solTrainX,solTrainY)
rowN <- nrow(sol.dat)
idxx <- sample(rowN, rowN/2,replace=FALSE)
trainX <- sol.dat[idxx,];dim(trainX)
testX <- sol.dat[-idxx,];dim(testX)

lm.out4 <- lm(solTrainY ~., trainX)
summary(lm.out4)$r.squared

pred.testX <- predict(lm.out4,testX)
cor(pred.testX, testX$solTrainY,use="complete.obs")**2

```

When sample size is the problem, in comparison to having ample predictors, bootstrapping might be the best solution.

Sampling w/ replacement -- predict on whatever samples were not sampled in first part
-- generally not as good when sample sizes are small

```{r}

bootSplits <- createResample(sol.dat$solTrainY,times=2,2)
str(bootSplits)

lm.out44 <- lm(solTrainY ~., sol.dat[bootSplits[[1]],])
summary(lm.out44)$r.squared
pred.testX2 <- predict(lm.out44,sol.dat[bootSplits[[2]],])
cor(pred.testX2, sol.dat[bootSplits[[2]],]$solTrainY,use="complete.obs")**2

```


#Use of the train() function

General framework for running data mining algorithms from the caret package.

The train() function from the caret package is a general wrapper where you can using CV, bootstrapping, do pre-processing and run almost all of the statistical learning algorithms from all of the popular packages.

Can use very advanced methods such as support vector machines.

Using our simulated random noise from before
```{r}
# ?train
# ?trainControl

my.control <- trainControl(method="cv")

out.tr <- train(y.reg ~., data=train,method="svmRadial",trControl=my.control)
out.tr

```

So that brings us back down to reality. Two important things to keep in mind. 1. with some methods, it can be really easy to overfit your data, meaning that it won't generalize. 2. This is especially pertinent when the number of variables is large in relation to number of respondents. 


Let's go into easier ways to either use cross-validation or bootstrapping to prevent over-fitting

Using the caret package primarily

## Whether to use Cross-Validation or Bootstrapping?

No right answer, and in most cases give very similar results. When sample size is an issue, bootstrapping may do better, as the resulting sample sizes for each bootstrap sample = starting N.

See pages 69-73 in Applied Predictive Modeling

![bootstrapping](/Users/RJacobucci/Dropbox/ATI Data Mining Labs/boot.png) 

![K-fold Cross-Validation](/Users/RJacobucci/Dropbox/ATI Data Mining Labs/cv.png) 


Just easier to leave dataset and specify within the actual prediction script

How do set up parallel with caret:
http://topepo.github.io/caret/parallel.html

```{r,eval=FALSE}
 library(doSNOW)
 getDoParWorkers()
 cl <- makeCluster(4, type="SOCK")
 registerDoSNOW(cl)


 my.control <- trainControl(method="cv",allowParallel=T)



system.time(penal.out1 <- train(y.reg ~., train,method="penalized",
                                trControl = my.control,
                               tuneLength=5))
penal.out1
# quartz()
 plot(penal.out1)
```

See that maybe the tuning parameters that were used, defaults, don't capture the potential "best" model -- see the the lowest RMSE is at the edge of the graph

I would either specify own tune grid, which I will demonstrate in part 3, or would just specify a wider tuneLength (15 or so)






